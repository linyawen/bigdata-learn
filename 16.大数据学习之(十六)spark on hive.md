# SqarkSql、hive

> HQL是大数据生态中最重要的，大部分大数据产品都兼容HQL 
>
> 所以必须要熟练掌握HQL的DDL，DQL，DML

**基于idea调试环境，测试 sparkSql连接hive。**

## 一、jdbc连mysql

可以用jdbc 连接 mysql读入数据，然后用HQL 操作，再通过jdbc  写回mysql数据，不过大公司更倾在跟mysql交互之间通过文件(csv等)中转，实现解耦、批量处理加速。

> 注意，如果sql会导致shuffle（比如 join操作），那需要注意设置并行度为1
>
> 否则默认200，会执行很200个task，很慢。
>
> 写回mysql注意写入策略：SaveMode.Overwrite 、SaveMode.Append 、SaveMode.ErrorIfExists、SaveMode.Ignore

### mysql 准备



```sql
-- 建表 学生、成绩表
CREATE TABLE `users` (
  `name` varchar(255) DEFAULT NULL,
  `id` int(11) NOT NULL,
  `age` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
CREATE TABLE `score` (
  `id` int(11) NOT NULL,
  `score` int(255) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
-- 插入数据
insert into users values (1,"小明1",18), (2,"小红",19), (3,"小王",20);
insert into score values (1,98), (2,99), (3,100);
```

### 编写sparkSQL

```xml
  <!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java -->
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>5.1.48</version>
        </dependency>
```

读取mysql，执行HQL计算后，结果写回myql。

```scala

    val ss: SparkSession = SparkSession
      .builder()
      .appName("sdfsf")
      .master("local")
      .config("spark.sql.shuffle.partitions", "1")  //默认会有200并行度的参数  ，时shuffle
      .getOrCreate()
    val sc: SparkContext = ss.sparkContext
    sc.setLogLevel("INFO")

    val pro = new Properties ()
    pro.put("url","jdbc:mysql://192.168.56.102/spark")
    pro.put("user","root")
    pro.put("password","Ylz@Yhkj#2020")
    pro.put("driver","com.mysql.jdbc.Driver")
    
     val usersDF: DataFrame = ss.read.jdbc(pro.get("url").toString,"users",pro)
    val scoreDF: DataFrame = ss.read.jdbc(pro.get("url").toString,"score",pro)

    usersDF.createTempView("userstab")
    scoreDF.createTempView("scoretab")
	//HQL 执行负载计算操作
    val resDF: DataFrame = ss.sql("select  userstab.id,userstab.name,userstab.age, scoretab.score   from    userstab join scoretab on userstab.id = scoretab.id")
    resDF.show()
    resDF.printSchema()

//    println(resDF.rdd.partitions.length)
//    val resDF01: Dataset[Row] = resDF.coalesce(1)
//    println(resDF01.rdd.partitions.length)
	//把HSQL的结果集写回mysql
    resDF.write.jdbc(pro.get("url").toString,"bbbb",pro)
```

## 二、SparkSql on  hive 之 本地文件系统

 如下demo展示了spark HQL基本操作:

1. 基于spark 内置 的metaStore(derby db)。
2. 用本地文件系统代替hive的hdfs。

### 操作单机版Hive

测试DDL，DML，DQL


```scala
val ss: SparkSession = SparkSession
      .builder()
      .master("local")
      .appName("sdsfd")
      .config("spark.sql.shuffle.partitions", 1) //注意，如果sql会导致shuffle（比如 join操作），那需要注意设置并行度为1
      .config("spark.sql.warehouse.dir", "d:/spark/warehouse")
      .enableHiveSupport()   //开启hive支持   ？   自己会启动hive的metastore
      .getOrCreate()
    val sc: SparkContext = ss.sparkContext
//    sc.setLogLevel("ERROR")
    import ss.sql
    /* ---------- DDL ---------------*/
    println("---在默认的default库下--------------------")
    sql("create database msb")
    sql("create table table01(name string)")  //作用再current库
    println("---在msb库下-----------------------------")
    sql("use msb")
    sql("create table table02(name string)") 
    /*---------- DML ---------------*/
    ss.sql("insert into table01 values ('zhangsan'),('lisi')")
    sql("insert into msb.table02 values ('zhangsan'),('lisi')")
    sql("use msb")
    sql("insert into  table02 values ('zhangsan2'),('lisi2')")
    
    /*---------- DQL ---------------*/
    sql("select * from  table01").show()
    sql("select * from  msb.table02").show()
```

> 在默认库default下建表table01，msb库下建表 table02，建库和表都是建立目录。
> 在工程目录下可以看到 spark内置的metaStore用的derby数据库。

![image-20230205174245841](D:\学习资料\大数据学习笔记\static\sparkSql_local_1.png)
![image-20230205174338691](D:\学习资料\大数据学习笔记\static\sparkSql_local_2.png)
![image-20230205180104706](D:\学习资料\大数据学习笔记\static\sparkSql_local_3.png)

### FAQ

1. Unable to locate hive jars to connect to metastore. Please set spark.sql.hive.metastore.jars

   解决：jdk11造成，改成jdk8

## 三、SparkSql on  hive  之 Hdfs

如下demo展示了sparkSQL 基于hive 分布式集群的基本操作。

1. 连接 hive metaStore 服务。
2. 基于hive hdfs进行 DDL，DML操作。

### 0.启动hadoop/hive相关服务

- hadoop-hdfs集群，

- hive-metastore,

- hiveserver2

  （这个不是必须，只是便于jdbc 执行HQL）

### 1.Hadoop_user_name

把idea里 hadoop的用户变量设置为 hdfs的启动用户。

![](D:\学习资料\大数据学习笔记\static\sparkSql_hdfs_3.png)

![](D:\学习资料\大数据学习笔记\static\sparkSql_hdfs_2.png)
### 2.DDL
如下图：

1. 红线1，连接hive metastore服务。
2. 红线2，把集群里 hadoop的配置文件复制过来，这样才能读到hdfs 有关的信息
3. 红线3，创建session临时表，hive里找不到这个表。
4. 红线4，创建hive表。

![](D:\学习资料\大数据学习笔记\static\sparkSql_hdfs_1.png)

如下图，用hive beeline 连进去执行sql查询， 没有ooxx表（因为是createTempView），但是有xxoo表，因为是执行HQL create table创建的表。

![](D:\学习资料\大数据学习笔记\static\sparkSql_hdfs_4.png)



### 3.执行demo

上图的demo，注释掉 DDL语句，打开DML的insert语句。

> 注意：这个 建表语句和 insert语句不能在一次应用提交运行，必须分两次分别运行，不然会报错。
>
> 暂时不知道为什么。

```
//   ss.sql("create table xxoo ( id int)")  //DDL

    ss.sql("insert into xxoo values (3),(6),(7)")  //DML  数据是通过spark自己和hdfs进行访问
//    df01.write.saveAsTable("oxox")


```



如下，用hive beeline 连进去执行sql查询，可以看到已经往hive表里插入了数据

![](D:\学习资料\大数据学习笔记\static\sparkSql_hdfs_5.png)





### FAQ

1. org.apache.hadoop.security.AccessControlException: Permission denied: user=dabao, 

   ```
   环境变量没有把Hadoop_user_name 设置为和hdfs一致。
   ```
   
   
   
2. could only be replicated to 0 nodes instead of minReplication (=1). There are 3 datanode(s) running 3 node(s) are excluded in this operation. 

   
   ```shell
    #主要是因为 idea dfs client和datanode网络不通，3 个datanode都连不上去所以提示 3 nodes are excluded
    #可能有多种原因，有可能是防火墙，也可能是idea dfs client的集群内部ip导致。
    #在hdfs-site.xml 需要加上，并重启 stop-all.sh、start-all.sh
    #https://hadoop.apache.org/docs/r2.6.5/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml
   	<property>
   		<name>dfs.datanode.use.datanode.hostname</name>
   		<value>true</value>
   	</property>
   	<property>
   	#最主要是这一句
   		<name>dfs.client.use.datanode.hostname</name>
   		<value>true</value>
   	</property>
	<property>
   		　　<name>dfs.permissions</name>
   		　　<value>false</value>
   	</property>
   ```
   



## 四、进阶-SQL函数

### 0.准备数据

基于如下数据集进行udf试验:

```scala
  val dataDF: DataFrame = List(
      ("A", 1, 90),
      ("B", 1, 90),
      ("A", 2, 50),
      ("C", 1, 80),
      ("B", 2, 60)
    ).toDF("name", "class", "score")

    dataDF.createTempView("users")
```

### 1. udf-作用在每条数据上

如下例子自定义一个函数，然后再SparkSQL中使用，相比hive on hadoop那种 必须要接口、编译出jar包、上传hdfs才能使用的先进很多！！

```scala
   //sparkSession中用匿名函数注册自定义sql函数，作用在每行数据
   ss.udf.register("ooxx",(x:Int)=>{x*10})
   // SparkSql中立刻使用它
    ss.sql("select * ,ooxx(score) as ox from users").show()
```

结果显示如下:

```shell
+----+-----+-----+---+
|name|class|score| ox|
+----+-----+-----+---+
|   A|    1|   90|900|
|   B|    1|   90|900|
|   A|    2|   50|500|
|   C|    1|   80|800|
|   B|    2|   60|600|
+----+-----+-----+---+
```

### 2.udaf作用在整组数据上

聚合函数需要实现接口,如下范例自定义平均分函数

```scala
    ss.udf.register("ooxx",new MyAggFun)  // avg
    ss.sql("select name,    " +
      " ooxx(score)    " +
      "from users  " +
      "group by name  ")
      .show()

```

```scala
class MyAggFun extends   UserDefinedAggregateFunction {
  override def inputSchema: StructType = {
    // ooxx(score)
    StructType.apply(Array(StructField.apply("score",IntegerType,false)))
  }

  override def bufferSchema: StructType = {
    //avg  sum / count = avg
    StructType.apply(Array(
      StructField.apply("sum",IntegerType,false),
      StructField.apply("count",IntegerType,false)
    ))
  }

  override def dataType: DataType = DoubleType

  override def deterministic: Boolean = true

  override def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(0) = 0
    buffer(1) = 0
  }

  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    //组内，一条记录调用一次
    buffer(0) = buffer.getInt(0) +  input.getInt(0)  //sum
    buffer(1) = buffer.getInt(1) + 1  //count

  }

  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    //类似hadoop的通过map端combiner然后再reduce来减少IO，spark这边也是 update方法每条迭代时进行聚合统计，然后多个统计结果再次汇总。
    buffer1(0) = buffer1.getInt(0) + buffer2.getInt(0)
    buffer1(1) = buffer1.getInt(1) + buffer2.getInt(1)
  }

  override def evaluate(buffer: Row): Double = {  buffer.getInt(0) / buffer.getInt(1)  }
}
```
sql执行结果:
```scala
+----+---------------+
|name|myaggfun(score)|
+----+---------------+
|   B|           75.0|
|   C|           80.0|
|   A|           70.0|
+----+---------------+
```

### 3.开窗函数

关键词  over,partition by 。

> 跟 groupby 区别: groupby 后结果集里每个分组只有一条，而over是所有

```sql
   ss.sql("select * ," +
      " rank()  over(partition by class order by score desc  )  as rank ,   " +
      " row_number()  over(partition by class order by score desc  )  as number    " +
      "from users ").show()

```

# 注意

## 1. spark.sql.shuffle.partitions 

如果sql会导致shuffle（比如 join操作），那需要注意设置并行度为1，否则会卡。如下

```scala
.config("spark.sql.shuffle.partitions", 1) 
```



